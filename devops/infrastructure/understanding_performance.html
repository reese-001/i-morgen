<!DOCTYPE html>
<html>
	<head>
		<link rel="stylesheet" href="../styles.css">
		<title>i morgen</title>
		<link rel="icon" href="../images/logo.png">
	</head>
	<body>
		<table width="80%">
			<tr>
				<td width="5%"><img src="../images/logo.png" class="logo"></td>
				<td width="80%"><h1 style="color:hsl(241, 74%, 46%)">i morgen</h1></td>
				
			</tr>
		</table>
		
		<h2 style="color:#0A45A5;"><a href="../../index.html">home</a> : <a href="../index.html">devops</a> : <a href="index.html">infrastructure</a> : understanding performance</h2>
		
		<p><span style="font-size: 20px;"><strong>Understanding Performance</strong></span></p>


        <p><span style="font-size: 18px;"><strong>Contents</strong></span></p>
        <li><a href="#overview">Overview</a></li>
		<li><a href="#objectives">System Performance Objectives</a></li>
		<li><a href="#performance_measurement_metrics">Performance Measurement Metrics</a></li>
		<li><a href="#network_latency">Network Latency</a></li>
		<li><a href="#memory_latency">Memory Latency</a></li>
		
		

		<br/>
        <div id="overview">
            <p><span style="font-size: 18px;"><strong>Overview</strong></span></p>
		</div>
		<ul>
			<li>Definition</li>
			<ul>
				<li>Measure of how fast or responsive a system is under</li>
				<ul>
					<li>A given workload</li>
					<ul>
						<li>Backend data</li>
						<li>Request volume</li>
					</ul>
					<li>A given hardware configuration</li>
					<ul>
						<li>Kind of hardware</li>
						<li>Capacity of hardware</li>
					</ul>
					<li>During the process of measuring, both of these variables remain fixed</li>
				</ul>
			</ul>
				<li>Performance problems</li>
				<ul>
					<li>Every performance problem is a result of a queue developing somewhere</li>
					<ul>
						<li>Visualize a traffic jam</li>
						<li>Examples of queues</li>
						<ul>
							<li>Network socket queue</li>
							<li>DB IO queue</li>
							<li>OS run queue</li>
							<li>etc</li>
						</ul>
						
					</ul>
					<li>Reasons for queue build-up</li>
					<ul>
						<li>Inefficient slow processing</li>
						<li>Serial resource access</li>
						<li>Limited resource capacity</li>
					</ul>
				</ul>
				<li>Principles of addressing performance problems</li>
				<ul>
					<li>Addressing performance issues relates to the reasons they exist in the first place (and addressed above)</li>
				
					<li>Increase efficiency</li>
					<ul>
						<li>This relates to ensuring each request is handled efficiently and is not related to concurrency yet</li>
						<li>Efficient resource utilization</li>
						<ul>
							<li>IO</li>
							<ul>
								<li>Memory</li>
								<li>Network</li>
								<li>Disk</li>
							</ul>
							<li>Logic</li>
							<ul>
								<li>Algorithms</li>
								<li>DB queries</li>
							</ul>
							<li>Data storage</li>
							<ul>
								<li>Data structures</li>
								<li>DB schema</li>
							</ul>
							<li>Caching</li>
						</ul>
					</ul>
					<li>Increase concurrency</li>
					<ul>
						<li>Once we have confidence that each request is being handled efficiently, we can focus on concurrency</li>
						<li>Hardware</li>
						<li>Software</li>
						<ul>
							<li>Queuing</li>
							<li>Coherence</li>
							<ul>
								<li>Refers to the consistency of shared data across multiple processing units, such as threads or processors.</li>
							</ul>
						</ul>
					</ul>
					<li>Increase capacity</li>
					<ul>
						<li>We dont need to learn how to add capacity, it is just something we can do</li>
					</ul>
				</ul>				
			</ul>
		</ul>

		<br/>
        <div id="objectives">
            <p><span style="font-size: 18px;"><strong>System Performance Objectives</strong></span></p>
		</div>
		<ul>
			<li>There are two objectives</li>
			<ul>
				<li>Minimize request-response latency</li>
				<ul>
					<li>Latency is measured in time units</li>
					
					<li>Equals the sum of the cumulative</li>
					<ul>
						<li>Wait/idle time</li>
						<li>Processing time</li>
					</ul>
				</ul>
				
				<li>Maximize throughput</li>
				<ul>
					<li>Sum of how many requests a system can process in a given time</li>
					<li>Measured as a rate of request processing</li>
					<li>Equals the sum of the cumulative</li>
					<ul>
						<li>Latency</li>
						<li>Capacity</li>
					</ul>
					
				</ul>
			
			</ul>
			<li>Request/response latency</li>
			<ul>
				<li>Most processes will be request/response</li>
				<ul>
					<li>Web application</li>
					<li>Database</li>
				</ul>
				<li>Batch processing is not request/response</li>
				<ul>
					<li>We do not measure request/response latency, only throughput </li>
				</ul>
			</ul>
			<li>Since adding capacity is external to this conversation, we can state that our goal is to decrease latency, which will increase throughput</li>
		</ul>

		<br/>
        <div id="performance_measurement_metrics">
            <p><span style="font-size: 18px;"><strong>Performance Measurement Metrics</strong></span></p>
		</div>
		<ul>
			<li>There are many possible metrics that can be measured, however four of the most important ones are</li>
			<ul>
				<li>Latency</li>
				<ul>
					<li>Affects: User Experience</li>
					<ii>Desired: As low as possible</ii>
				</ul>
				<li>Throughput</li>
				<ul>
					<li>Affects: Number of users that can be supported</li>
					<ii>Desired: Greater then the request rate</ii>
				</ul>
				<li>Errors</li>
				<ul>
					<li>Measured in the percentage of errors</li>
					<li>Affects: Functional correctness</li>
					<li>Desired: None</li>
					<li>Note: Some time out errors are ok, but functional errors should be avoided </li>
				</ul>
				<li>Resource Saturation</li>
				<ul>
					<li>Measured in the percent of resouce completely utilized</li>
					<li>Affects: Hard capacity required</li>
					<ii>Desired: Efficient utilization of all system resources</ii>
				</ul>
			</ul>
			<li>Tail latency</li>
			<ul>
				<li>Tail latency is high percentile latency, representing requests that have a response time longer than some upper bound (typical == 99%) of all requests handled by a service or application</li>
				<li>Is the experience for users experiencing the tail latency acceptable?</li>
				<li>Increases in workload typically result in higher number of users will experience worse performance</li>
				<ul>
					<li>Tail latency is typically the result of queuing</li>
					<li>More users will experience queuing as workload increases</li>
				</ul>
				<li><a href="../images/tail_latency.jpg" target="_blank">Graphic</a></li>
			</ul>
		</ul>

		<br/>
		<div id="network_latency">
            <p><span style="font-size: 18px;"><strong>Network Latency</strong></span></p>
		</div>
		<ul>

	
		<li>Defined in terms of</li>
		<ul>
			<li>End user to front end</li>
			<li>Front end to backend resources</li>
		</ul>
		<li>Connection latency is related to</li>
		<ul>
			<li>TCP connections <a href="../../system_design/protocols.html#tcp" target="_blank">link</a> connections</li>
			<li>Three way handshake <a href="../images/three_way_handshake.jpg" target="_blank">graphic</a></li>
		</ul>
		<li>Methods to address network latency</li>
		<ul>
			<li>Connection pooling</li>
			<ul>
				<li>Creating connections is expensive</li>
				<li>Pooling allows the reuse of connections</li>
				<li>Applications, upon initialization, will create a pool of connections (http, db, etc)</li>
			</ul>
			<li>Persistant connections</li>
			<ul>
				<li>This is built into the http 1.1 protocol and automatically implemented</li>
				<li>When used with connection pooling, this further improves the performance of the http client</li>
				<li>Also known as keep-alive connections</li>
			</ul>
		
		</ul>
		<li>Methods to address data transfer latency</li>
		<ul>
			<li>Avoid data transfer when possible</li>
			<ul>
				<li>Cache data appropriately either on client or server side</li>
				<li>Caching occurs on the browser side by storing static data (images, etc) locally</li>
			</ul>
			<li>Use efficient data formats</li>
			<ul>
				<li>For example when communicating to a REST API</li>
				<ul>
					<li>Use RPC instead of HTTP protocols where possible</li>
					<li>Negative: they limit interoperability</li>
					<li>Note: by doing this, your REST server becomes an RPC server</li>
				</ul>
			</ul>
			<li>Use compression</li>
			<ul>
				<li>Reduces the transmitted data at the expense of actually compressing and uncompressing it</li>
				<li>Generally that overhead is not as costly as the data overhead</li>
			</ul>
			
		</ul>
	</ul>


		<br/>
		<div id="memory_latency">
            <p><span style="font-size: 18px;"><strong>Memory Latency</strong></span></p>
		</div>
		<ul>
			<li>Possible Issues</li>
			<ul>
				<li>Finite heap memory</li>
				<ul>
					<li>Memory is always finite and can be exceeded</li>
					<li>In the event heap memory becomes contrained, GC will become aggressive and slow down the system</li>
				</ul>
				<li>Large heap memory</li>
				<ul>
					<li>If heap memory > physical memory, swapping can occur and severely slow performance of a process</li>
					<li>This can also create a scenario where GC becomes slow as it needs to sweep a larger area</li>
				</ul>
				<li>GC algorithm</li>
				<ul>
					<li>Different algorithms are optimized for different scenarios</li>
					<li>Failure to identify the correct algorithm will reduce speed</li>
				</ul>
				<li>Database memory constraints</li>
				<ul>
					<li>R/W operation in the database typically takes place within a buffer and written back to disk</li>
					<li>As such, space utilization of DB buffer memory is important </li>
				</ul>
			</ul>
			<li>Methods to control memory issues</li>
			<ul>
				<li>Avoid memory bloat</li>
				<ul>
					<li>This is often outside of our control</li>
					<li>Should consume as little heap memory as possible</li>
				</ul>
				<li>Weak and soft references</li>
				<ul>
					<li></li>
				</ul>
			</ul>
		</ul>
		

	</body>
</html>