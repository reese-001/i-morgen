<!DOCTYPE html>
<html>
	<head>
		<link rel="stylesheet" href="../styles.css">
		<title>i morgen</title>
		<link rel="icon" href="../images/logo.png">
	</head>
	<body>
		<table>
			<tr>
				<td><img src="../images/logo.png" class="logo"></td>
				<td><h1 style="color:#0B2042;">i morgen</h1></td>
			</tr>
		</table>
		
		<h2 style="color:#0A45A5;"><a href="../../index.html">home</a> : <a href="../index.html">data science</a> : sagemaker : intro</h2>
		
        <p><span style="font-size: 18px;"><strong>Contents</strong></span></p>
        <li><a href="#what_is_it">What Is It</a></li>
		<li><a href="#linear_learner">Linear Learner</a></li>
		<li><a href="#xgboost">XGBoost</a></li>
		
        <div id="what_is_it">
            <p><span style="font-size: 18px;"><strong>What Is It</strong></span></p>
		</div>
		<ul>
			<li>Intended to manage the entire machine learning workflow - <a href="images/workflow.png">image</a></li>
			<ul>
				<li>Start by fetching, cleaning data and prepare data</li>
				<li>Feed that into a model to train and evaluate</li>
				<li>Deploy model to make inferences on data we have not seen</li>
				<li>Monitor the data in production and the cycle repeats</li>
			</ul>
			<br/>
			<li>Architecture</li>
			<ul>
				<li>Training</li>
				<ul>
					<li>Training data will reside in an S3 bucket</li>
					<li>Sagemaker will provision a number of training hosts</li>
					<li>The code that is uses comes from a Docker image (training code image) that is registered in <a href="../../aws/docker/docker.html" target="_blank">ECR</a> </li>
					<li>The output data will be saved in a different S3 folder</li>
				</ul>	
				<li>Deployment</li>
				<ul>
					<li>There will be another Docker image in ECR called Inference Code Image</li>
					<li>This ECR image will potentially be much simplier as its only job is to take incoming requests and make inferences based on the incoming data</li>
				</ul>
				<li>Endpoints</li>
				<ul>
					<li>Used to connect the model deployment image(s) to the outside world</li>
				</ul>
			</ul>
			<br/>
			<li>Using Notebooks to direct the process</strong></li>
			<ul>
				<li>Sagemaker notebook has access to S3 and can interface with the datasets being used</li>
				<li>Within the notebook you can implement scikit_learn, spark, tensorflow, etc</li>
				<li>Has access to a wide variety of prebuilt models in the form of Docker images</li>
				<ul>
					<li>Within the notebook itself, you can create an entire fleet of instances from these Docker images to execute your training on</li>
					<li>When the training is complete, you can repeat this process for deployment and ultimately have a large fleet of images for inferencing</li>
				</ul>
				<li>Can create hyperparameter tuning jobs</li>
			</ul>
			<br/>
			<li>Data preperation</li>
			<ul>
				<li>Sagemaker expects your data to come from S3</li>
				<ul>
					<li>It is reasonable to prepare the data externally to this process and deliver that data to S3, or </li>
					<li>You can execute your data prep steps directly in the notebook</li>
					<li>Most performant data will be in RecordIO/Protobuf format</li>
					<ul>
						<li>RecordIO is a binary format used to store large amounts of data in a compact and efficient way</li>
						<li>Protobuf is short for Protocol Buffers</li>
						<ul>
							<li>Language neutral data serialization format used for transmitting data over networks or storing it in files</li>
						</ul>
					</ul>
					<li>CSV is also an accepted format</li>
				</ul>
				<li>Other input sources are Athena, EMR, Redshift, Amazon Keyspaces DB</li>
				<li>Spark integrates with Sagemaker for large scale processing </li>
			</ul>
			<br/>
			<li>Training on Sagemaker</li>
			<ul>
				<li>Training jobs can be created from your notebook or console</li>
				<li>The following data will be required</li>
				<ul>
					<li>URL of the S3 location with your training data</li>
					<li>ML computer resources</li>
					<li>URL of the S3 bucket for output</li>
					<li>ECR path to training code</li>
				</ul>
				<li>Large library of training options</li>
				<ul>
					<li>Built in options</li>
					<li>Spark MLLib</li>
					<ul>
						<li>Spark MLlib is a machine learning library for the Apache Spark platform</li>
						<li>It provides a wide range of machine learning algorithms and utilities for data processing and analysis, including classification, regression, clustering, recommendation, and feature extraction</li>
					</ul>
					<li>TensorFlow</li>
					<ul>
						<li>TensorFlow is an open-source software library for dataflow and differentiable programming across a range of tasks</li>
						<li>It is a symbolic math library, and is also used for machine learning applications such as neural networks</li>
					</ul>
					<li>MXNet</li>
					<ul>
						<li>Pronounced "Mix-net"</li>
						<li>Open source deep learning framework developed by Apache Software Foundation</li>
						<li>It is designed to be highly efficient and flexible, allowing for easy deployment of deep learning models on a variety of devices, including CPUs, GPUs, and mobile devices</li>
						<li>MXNet supports a wide range of programming languages, including Python, R, C++, and Julia</li>
					</ul>
					<li>Your own Docker image</li>
					<li>Algorithm purchased from AWS marketplace</li>
				</ul>
			</ul>
			<br/>
			<li>Deploying on Sagemaker</li>
			<ul>
				<li>Can be done from the notebook</li>
				<li>Can deploy in two ways</li>
				<ul>
					<li>Request Sagemaker to create persistent endpoints for making individual inferences on demand</li>
					<li>Request Sagemaker to create a Batch Transform to get predictions for an entire dataaset</li>
				</ul>
				<li>Other options include</li>
				<ul>
					<li>Inference Pipelines</li>
					<ul>
						<li>A way to organize and deploy a sequence of models as a single endpoint</li>
					</ul>
					<li>Sagemaker Neo</li>
					<ul>
						<li>Allows you to deploy your models all the way to edge devices via Cloudfront</li>
						<li>When you deploy a model using SageMaker Neo, the model is optimized for the target device, and then Neo uses <a href="../../aws/cloudfront/cloudfront.html" target="_blank">CloudFront</a> to distribute the model to the edge locations closest to the devices where the model will be used</li>
					</ul>
					<li>Elastic Inference</li>
					<ul>
						<li>Allows users to add GPU acceleration to their Amazon Elastic Compute Cloud (EC2) instances on an as-needed basis</li>
					</ul>
					<li>Automatic scaling</li>
					<ul>
						<li>Increases the number of endpoints as needed</li>
					</ul>
				</ul>
			</ul>
		</ul>
		<div id="linear_learner">
            <p><span style="font-size: 18px;"><strong>Linear Learner</strong></span></p>
		</div>
		<ul>
			<li>Linear learner can accomodate both regression and classification predictions</li>
			<li>Input can be either file or pipped and can be one of the following formats</li>
			<ul>
				<li>RecordIO wrapped protobuf (float32 data only)</li>
				<li>CSV. First column assumed to be the label</li>
				<li>Hint: If you experience issues training data located in an S3 bucket, consider pipe mode </li>
			</ul>
			<br/>
			<li>How is it used?</li>
			<ul>
				<li>Preprocessing</li>
				<ul>
					<li>Training data must be normalized</li>
					<ul>
						<li>You can do this yourself or Linear Learner can do it</li>
					</ul>
					<li>Input should be shuffled</li>
				</ul>
				<li>Training</li>
					<li>Available optimization algorithms</li>
					<ul>
						<li>Adam</li>
						<li>AdaGrad</li>
						<li>SGD</li>
					</ul>
					<li>Multiple models trained in parallel</li>
					<li>Tune L1, L2 regularization</li>
					
					<li>Validation</li>
					<ul>
						<li>Most optimal model is selected</li>
					</ul>
				</ul>
			
				<br/>
				<li>Hyperparameters</li>
				<ul>
					<li>Balance_multiclass_weights</li>
					<ul>
						<li>Gives each class equal importance in loss functions</li>
					</ul>
					<li>learning_rate, mini_batch_size</li>
					<li>L1: aka regularization</li>
					<li>L2: aka weight decay</li>
				</ul>
				<br/>
				<li>For instance types, keep in mind the following two guidelines</li>
				<ul>
					<li>Single or multi-machine CPU or GPU</li>
					<li>Multi-GPU on one machine does not help</li>
				</ul>
			</ul>
		</ul>
		<div id="xgboost">
            <p><span style="font-size: 18px;"><strong>XGBoost</strong></span></p>
		</div>
		
		<ul>
			<li>eXtreme Gradiant Boosting</li>
			<ul>
				<li>Boosted group of decision trees. See <a href="../core/bagging_vs_boosting.html" target="_blank">link</a> for more information</li>
			</ul>
			<li>Can be used for classification and regression using regression trees</li>
			<li>XGBoost was not made for Sagemaker in the original implementation, instead AWS started from the open source version of XGBoost that everyone else uses and built on top of that</li>
			<li>Historically has only accepted CSV or libsvm formatted data as input, but now takes recordIO-protobuf and Paraguet as well</li>
			<br/>
			<li>How is it used? </li>
			<ul>
				<li>Models are serialized/deserialized with Pickle</li>
				<li>Can use as a framework within notebooks with Sagemaker.xgboost</li>
				<li>Or as a built-in Sagemaker algorithm</li>
			</ul>
			<br/>
			<li>Hyperparameters (not inclusive!)</li>
			<ul>
				<li>Subsample</li>
				<ul>
					<li>Prevents overfitting</li>
				</ul>
				<li>Eta</li>
				<ul>
					<li>Step size shrinkage, prevents overfitting</li>
				</ul>
				<li>Gamma</li>
				<ul>
					<li>Gamma</li>
				</ul>
				<li>Alpha</li>
				<ul>
					<li>L1 regularization term; larger = more conservative</li>
				</ul>
				<li>Lambda</li>
				<ul>
					<li>L2 regularization term; larger = more conservative</li>
				</ul>
				<li>eval_metric</li>
				<ul>
					<li>Focus on accuracy: optimize for error or rmse</li>	
					<li>Focus on false positives: optimize for AUC</li>
				</ul>
				<li>scale_pos_weight</li>
			</ul>

		</ul>
		

	</body>
</html>