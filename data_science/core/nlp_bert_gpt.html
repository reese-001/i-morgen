<!DOCTYPE html>
<html>
	<head>
		<link rel="stylesheet" href="../styles.css">
		<title>i morgen</title>
		<link rel="icon" href="../images/logo.png">
	</head>
	<body>
		<table>
			<tr>
				<td><img src="../images/logo.png" class="logo"></td>
				<td><h1 style="color:#0B2042;">i morgen</h1></td>
			</tr>
		</table>
		
		<h2 style="color:#0A45A5;"><a href="../../index.html">home</a> : <a href="../index.html">data science</a> : nlp with bert and gpt</h2>
		
		<p><span style="font-size: 20px;"><strong>NLP with BERT and GPT</strong></span></p>


        <p><span style="font-size: 18px;"><strong>Contents</strong></span></p>
        <li><a href="#overview">Overview</a></li>
		<li><a href="#transfer_learning">Transfer Learning</a></li>
		
		<br/>

        <div id="overview">
            <p><span style="font-size: 18px;"><strong>Overview</strong></span></p>
		</div>
		<ul>
			<li>Transformer deep learning architectures are what is hot</li>
			<ul>
				<li>Transformer deep learning architectures are a type of neural network that are designed to process sequences of data, such as text, speech, or time series</li>
				<li>Transformers use self-attention mechanisms to dynamically weigh the importance of different elements in the input sequence, allowing the network to capture long-range dependencies in the data.</li>
				<li>Transformers are based on the idea of using multiple parallel attention layers, rather than sequential recurrent or convolutional layers, to process the input sequence. This allows for faster training and inference compared to traditional RNNs and CNNs</li>
				<li>Transformer architectures have been successful in a wide range of NLP tasks, including machine translation, text classification, and question answering, and have become a cornerstone of modern NLP models</li>
				<li>Examples include</li>
				<ul>
					<li>BERT</li>
					<ul>
						<li>A pre-trained language representation model developed by Google</li>
						<li>One of the key features of BERT is its bidirectional training process</li>
						<li>BERT is a Transformer-based model, and its architecture allows for parallel computation, making it highly scalable and efficient</li>
					</ul>

					<li>RoBERTa</li>
					<ul>
						<li>Robustly Optimized BERT Pretraining Approach</li>
						<li>Transformer based language model developed by Facebook AI in 2019</li>
						<li>RoBERTa aims to address some of the limitations of BERT, such as the limited amount of training data, and the way BERT was fine-tuned for specific NLP tasks</li>
						<li>Has become a popular choice for transfer learning in NLP</li>
					</ul>
					<li>T5</li>
					<ul>
						<li>Test-to-Test Transfer Tranformer</li>
						<li>Large-scale language model developed by Google AI in 2020</li>
						<li>Trained on a diverse range of tasks that involve generating text, such as translation, summarization, and question answering</li>
					</ul>
					<li>GPT-2</li>
					<ul>
						<li>Language model developed by OpenAI</li>
						<li>It is based on the Transformer architecture and uses a large corpus of text data to generate natural language text</li>
						<li>It is one of the largest language models developed to date, with over 1.5 billion parameters</li>
						<li>Primarily used to generate text</li>
					</ul>
					<li>DistilBERT</li>
					<ul>
						<li>Created by the Hugging Face team</li>
						<li>Uses knowledge distiliation to reduce model size by 40%</li>
						<li>The reduced size of DistilBERT makes it a popular choice for use in real-world applications where computational resources are limited</li>
					</ul>
				</ul>
			</ul>
			<li>BERT: (Bidirectional Encoder Representations from Transformers)</li>
			<ul>
				<li>A transformer-based deep learning model that has been pre-trained on a large corpus of text data to generate contextualized word representations</li>
				<li>BERT is designed to capture the meaning of words in context and to generate dense representations that capture the relationships between words in a sentence</li>
			</ul>
			<li>GPT: Generative Pre-trained Transformer</li>
		</ul>

		<br/>

        <div id="transfer_learning">
            <p><span style="font-size: 18px;"><strong>Transfer Learning</strong></span></p>
		</div>
		<ul>
			<li>These models are too big and complex to build from scratch</li>
			<li>The idea is to build off someone elses work</li>
			<li>Model zoos such as Hugging Face offer pre-trained models to start from</li>
			<ul>
				<li>This is integrated with Sagemaker via Hugging Face Deep Learning Containers (HFDLC)</li>
			</ul>
			<li>There are a few different options</li>
			<ul>
				<li>Fine Tuning</li>
				<ul>
					<li>Use this if you want to add a smaller amount of data to an existing large model</li>
					<li>Use a low learning rate to ensure you incrementally improve the model</li>
				</ul>
				<li>Add new layers to the top of a frozen model</li>
				<ul>
					<li>Learns to turn old features into predictions on new data</li>
					<li>You can do both as well, add new layers and then fine tune the complete model</li>
				</ul>
				<li>Retrain from scrath</li>
				<ul>
					<li>If you want to take advantage of the model infrastructure and also have large amounts of training data</li>
					<li>This is an expensive option</li>
				</ul>
			</ul>
			<li>BERT example</li>
			<ul>
				<li>Hugging Face offers a Deep Learning Container (DLC) for BERT</li>
				<li>Its pre-trained on BookCorpus and Wikipedia</li>
				<li>You can fine tune BERT (or DistilBERT, etc) with your own data</li>
				<li>You must tokenize the data in the same method the model was originally tokenized</li>
			</ul>
		</ul>	
`
	


	</body>
</html>